{"cells":[{"cell_type":"markdown","metadata":{"id":"yx8tPIntNqdR"},"source":["# GRU4REC Model: Preprocessing + Modelling\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2029,"status":"ok","timestamp":1743457469582,"user":{"displayName":"Nazhif Haidar","userId":"10254950625537949520"},"user_tz":-420},"id":"g-x7FLiacm1z","outputId":"8fbf22a3-db51-4381-fa2a-12ae56b3fe2a"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[31mERROR: You must give at least one requirement to install (see \"pip help install\")\u001b[0m\u001b[31m\n","\u001b[0m"]}],"source":["!pip install"]},{"cell_type":"markdown","metadata":{"id":"lctyVZRfN91b"},"source":["## Import Library"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2SfCEONnOBSP"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from tqdm import tqdm\n","import gdown\n","import joblib\n","from torch.optim import Adam\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.model_selection import train_test_split\n","from torch.nn.utils.rnn import pad_sequence\n","from torch.optim import Adam"]},{"cell_type":"markdown","metadata":{"id":"SJoRYVQdOt67"},"source":["## Angkut berkas dan ubah menjadi sequence yang tepat"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1sAfE6RbcdCZ"},"outputs":[],"source":["def download_drive_file(file_id, output_file):\n","    try:\n","        url = f'https://drive.google.com/uc?id={file_id}'\n","\n","        gdown.download(url, output_file, quiet=False)\n","\n","        print(f\"File downloaded successfully: {output_file}\")\n","    except Exception as e:\n","        print(f\"An error occurred while downloading the file: {e}\")\n","\n","# download the cleaned dataset\n","# download_drive_file('1LN4teEXs1tQ2nXgrKoRrZqVr9DVftUJh', 'sessions.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6-BMuDX3ch9a"},"outputs":[],"source":["# sessions = pd.read_csv('sessions.csv')\n","# sessions.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5936,"status":"ok","timestamp":1743457492182,"user":{"displayName":"Nazhif Haidar","userId":"10254950625537949520"},"user_tz":-420},"id":"0kdkeYC3ckcD","outputId":"a1665e18-93bb-419f-e4ec-64a575091db6"},"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading...\n","From: https://drive.google.com/uc?id=1dMKbq9sawiAVH9Z4Nh2c2ZsZ3nle1-1Q\n","To: /content/label_encoder.joblib\n","100%|██████████| 10.0M/10.0M [00:00<00:00, 37.9MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["File downloaded successfully: label_encoder.joblib\n"]}],"source":["download_drive_file('1dMKbq9sawiAVH9Z4Nh2c2ZsZ3nle1-1Q','label_encoder.joblib')\n","label_encoder = joblib.load('label_encoder.joblib')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":300},"executionInfo":{"elapsed":17566,"status":"ok","timestamp":1743457509760,"user":{"displayName":"Nazhif Haidar","userId":"10254950625537949520"},"user_tz":-420},"id":"mKKyT1_qbbe9","outputId":"0e08c713-4b87-4fd7-d4ed-f5f0c765c771"},"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading...\n","From: https://drive.google.com/uc?id=1i-0mMl7B0ey-hHUHujGoCiclesJtrEM7\n","To: /content/sequences.csv\n","100%|██████████| 58.9M/58.9M [00:01<00:00, 44.3MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["File downloaded successfully: sequences.csv\n"]},{"output_type":"execute_result","data":{"text/plain":["   index                                  sequence\n","0      0  [489206, 489237, 142933, 188025, 188028]\n","1      1                  [133985, 138876, 315348]\n","2      2            [19817, 129680, 129680, 20979]\n","3      3                    [443147, 12620, 12621]\n","4      4     [55082, 55082, 57935, 359096, 139532]"],"text/html":["\n","  <div id=\"df-ce4bc970-9d8c-4879-8689-2036e7913745\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>index</th>\n","      <th>sequence</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>[489206, 489237, 142933, 188025, 188028]</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>[133985, 138876, 315348]</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>[19817, 129680, 129680, 20979]</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>[443147, 12620, 12621]</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>[55082, 55082, 57935, 359096, 139532]</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ce4bc970-9d8c-4879-8689-2036e7913745')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-ce4bc970-9d8c-4879-8689-2036e7913745 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-ce4bc970-9d8c-4879-8689-2036e7913745');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-77972785-4581-4cdf-aff3-14b62a4ffb63\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-77972785-4581-4cdf-aff3-14b62a4ffb63')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-77972785-4581-4cdf-aff3-14b62a4ffb63 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"sequences"}},"metadata":{},"execution_count":6}],"source":["download_drive_file('1i-0mMl7B0ey-hHUHujGoCiclesJtrEM7', 'sequences.csv')\n","sequences_a = pd.read_csv('sequences.csv')\n","sequences = sequences_a\n","sequences.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TnQS39Gwbkmx"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6zQy36Cvcoai"},"outputs":[],"source":["# load the serialized label encoder from pickle file\n","\n","import re\n","\n","def create_full_sequence(df: pd.DataFrame) -> list:\n","    # Process all rows at once using vectorized operations\n","    prev_items = df['prev_items'].str.replace(r\"[\\[\\]'\\n\\\"]\", '', regex=True).str.split()\n","    next_items = df['next_item'].values.astype(str)\n","\n","    # Combine using numpy for fast array operations\n","    all_items = prev_items + pd.Series(next_items).apply(lambda x: [x])\n","    return all_items.to_list()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a40pRBC6rKj5"},"outputs":[],"source":["# sequences = create_full_sequence(sessions)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aJHHdIqzrR2D"},"outputs":[],"source":["from joblib import Parallel, delayed\n","import numpy as np\n","from tqdm import tqdm\n","\n","# Precompute the dictionary mapping (fastest possible lookup)\n","mapping = {item: idx for idx, item in enumerate(label_encoder.classes_)}\n","\n","# Split data into chunks for parallel processing\n","def chunker(seq, chunk_size):\n","    return (seq[i:i+chunk_size] for i in range(0, len(seq), chunk_size))\n","\n","# Worker function for parallel processing\n","def process_chunk(chunk, mapping):\n","    return [[mapping[item] for item in sublist] for sublist in chunk]\n","\n","# Parallel execution with chunking and progress tracking\n","def parallel_transform(sequences, mapping, n_jobs=-1, chunk_size=1000):\n","    chunks = list(chunker(sequences, chunk_size))\n","    results = Parallel(n_jobs=n_jobs)(\n","        delayed(process_chunk)(chunk, mapping) for chunk in tqdm(chunks, desc=\"Processing chunks\", unit=\"chunk\")\n","    )\n","\n","    return [sublist for chunk in results for sublist in chunk]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"61mzY1fQrSti"},"outputs":[],"source":["# transformed_seq = parallel_transform(sequences, mapping)\n","import ast\n","transformed_seq = [ast.literal_eval(seq) for seq in sequences['sequence']]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":961,"status":"ok","timestamp":1743457532454,"user":{"displayName":"Nazhif Haidar","userId":"10254950625537949520"},"user_tz":-420},"id":"z8f9_W9BrWdS","outputId":"e2d02343-5427-4b38-bb7a-a82703133836"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[334229,\n"," 187696,\n"," 224931,\n"," 224971,\n"," 208412,\n"," 224931,\n"," 221934,\n"," 221932,\n"," 208412,\n"," 101219,\n"," 118212,\n"," 56406]"]},"metadata":{},"execution_count":11}],"source":["train_sequences, test_sequences = train_test_split(transformed_seq, test_size=0.1, random_state=42)\n","test_sequences, val_sequences = train_test_split(test_sequences, test_size=0.5, random_state=42)\n","np.random.choice(np.array(train_sequences, dtype=object))"]},{"cell_type":"markdown","metadata":{"id":"fnYiZCM7Ot3j"},"source":["## Dataset and Dataloader definition\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f05e--gFO1sL"},"outputs":[],"source":["class SessionDataset(Dataset):\n","    def __init__(self, sessions, session_ids):\n","        self.sessions = [torch.tensor(session, dtype=torch.long) for session in sessions]\n","        self.session_ids = session_ids  # Store session IDs to track session boundaries\n","\n","    def __len__(self):\n","        return len(self.sessions)\n","\n","    def __getitem__(self, idx):\n","        return self.sessions[idx], self.session_ids[idx]\n","\n","def collate_fn(batch):\n","    sessions, session_ids = zip(*batch)  # Separate sequences and session IDs\n","    padded_sessions = pad_sequence(sessions, batch_first=True, padding_value=0)\n","\n","    inputs = padded_sessions[:, :-1]   # All but last item as input\n","    targets = padded_sessions[:, 1:]   # All but first item as target\n","    mask = (inputs != 0).float()       # Mask for padding (0 means padded position)\n","\n","    return inputs, targets, mask, list(session_ids)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eRj-4e4P0Cqf"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"Vv3PkbSROtuo"},"source":["## Model Definition"]},{"cell_type":"markdown","metadata":{"id":"O4LUFRgeXH14"},"source":["## 1. GRU Layer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2Rqrz-8UXj4i"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","\n","class GRULayer(nn.Module):\n","    def __init__(self, input_size, hidden_size, output_size,\n","                 num_layers, batch_size, dropout_hidden_rate,\n","                 dropout_input_rate, optimizer=None):\n","        \"\"\"\n","        GRU Layer for the GRU4REC model.\n","\n","        Args:\n","            input_size (int): Input dimension size\n","            hidden_size (int): Hidden layer size\n","            output_size (int): Output dimension size\n","            num_layers (int): Number of GRU layers\n","            batch_size (int): Mini-batch size\n","            dropout_hidden_rate (float): Dropout rate for hidden layers\n","            dropout_input_rate (float): Dropout rate for input layer\n","        \"\"\"\n","        super(GRULayer, self).__init__()\n","\n","        self.hidden_size = hidden_size\n","        self.input_size = input_size\n","        self.output_size = output_size\n","        self.num_layers = num_layers\n","        self.batch_size = batch_size\n","        self.dropout_hidden_rate = dropout_hidden_rate\n","        self.dropout_input_rate = dropout_input_rate\n","        self.optimizer = optimizer\n","\n","        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","        # Dropout layers\n","        self.dropout_hidden = nn.Dropout(p=dropout_hidden_rate)\n","        self.dropout_input = nn.Dropout(p=dropout_input_rate)\n","\n","        # Linear transformation from hidden state to output\n","        self.hidden_to_output = nn.Linear(hidden_size, output_size)\n","\n","        # GRU layer\n","        self.gru = nn.GRU(32, hidden_size, num_layers,\n","                          dropout=dropout_hidden_rate if num_layers > 1 else 0,\n","                          batch_first=True)\n","\n","        self.tanh = nn.Tanh()\n","\n","        # Initialize one-hot buffer correctly\n","        # self.one_hot_buffer = torch.zeros(batch_size, output_size, dtype=torch.float).to(self.device)\n","        self.embedding = nn.Embedding(input_size, 32)\n","\n","        self.to(self.device)\n","\n","    def forward(self, input, hidden):\n","        \"\"\"\n","        Forward pass through the GRU layer.\n","\n","        Args:\n","            input (Tensor): Input batch of item indices (batch_size, seq_len)\n","            hidden (Tensor): Hidden state tensor\n","\n","        Returns:\n","            logit (Tensor): Output predictions\n","            hidden (Tensor): Updated hidden state\n","        \"\"\"\n","        embedded = self.embedding(input)\n","\n","        # Apply input dropout if in training mode\n","        if self.training and self.dropout_input_rate > 0:\n","            embedded = self.dropout_input(embedded)\n","\n","        # print(embedded.shape)\n","\n","        # GRU expects input in (batch, seq_len, input_size) format\n","        # embedded = embedded.unsqueeze(1)\n","        # print(embedded.shape)\n","\n","        # Forward through GRU\n","        output, hidden = self.gru(embedded, hidden)\n","        # print(output.shape)\n","        # print(hidden.shape)\n","\n","        # Reshape and apply output transformations\n","        # output = output.reshape(-1, output.size(-1))\n","        # print(output.shape)\n","        output = self.dropout_hidden(output)\n","        # print(output.shape)\n","        # output = torch.squeeze(output, dim=1)\n","        # print(output.shape)\n","        logit = self.tanh(self.hidden_to_output(output))\n","        # print(logit.shape)\n","\n","        return logit, hidden\n","\n","    def one_hot_encode(self, input):\n","        \"\"\"\n","        Convert input item indices into one-hot encoded vectors.\n","\n","        Args:\n","            input (Tensor): Tensor of item indices (batch_size,)\n","\n","        Returns:\n","            one_hot (Tensor): One-hot encoded representation of input (batch_size, output_size)\n","        \"\"\"\n","        # Ensure input is long type for indexing\n","        input = input.long()\n","\n","        # Ensure the one-hot buffer has the correct size\n","        one_hot = torch.zeros(input.shape[0], self.output_size, dtype=torch.float, device=self.device)\n","\n","        # Ensure index values are within valid range\n","        if input.max() >= self.output_size:\n","            raise ValueError(f\"Index value {input.max()} exceeds output size {self.output_size}\")\n","\n","        # Convert input to correct shape and scatter\n","        one_hot.scatter_(1, input.reshape(-1, 1), 1)  # Use view() instead of unsqueeze()\n","\n","        return one_hot\n","\n","    def init_hidden(self):\n","        \"\"\"\n","        Initialize the hidden state for the GRU.\n","\n","        Returns:\n","            hidden (Tensor): Initialized hidden state tensor\n","        \"\"\"\n","        return torch.zeros(self.num_layers, self.batch_size, self.hidden_size, device=self.device)\n"]},{"cell_type":"markdown","metadata":{"id":"Ps3n_9Tck-7D"},"source":["## GRU4REC Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CpZRj27fBf1z"},"outputs":[],"source":["\n","class GRU4REC:\n","    def __init__(self, input_size, hidden_size,\n","                 output_size, num_layers,\n","                 dropout_hidden_rate, dropout_input_rate, batch_size,\n","                 optimizer, loss):\n","        super(GRU4REC, self).__init__()\n","        self.optimizer = optimizer\n","        self.loss = loss\n","        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        self.gru = GRULayer(input_size=input_size,\n","                            hidden_size=hidden_size,\n","                            output_size=output_size,\n","                            num_layers=num_layers,\n","                            dropout_hidden_rate=dropout_hidden_rate,\n","                            dropout_input_rate=dropout_input_rate,\n","                            batch_size=batch_size,\n","                            optimizer=optimizer\n","                            )\n","        self.hidden = torch.zeros(num_layers, batch_size, hidden_size).to(self.device)\n","\n","    def train(self, train_loader, val_loader, epochs):\n","        train_losses = []\n","        val_losses = []\n","        val_metrics = {}\n","\n","        # define metrics\n","        test_metrics = {\n","            \"mrr_at_k\": None,\n","            \"hit_at_k\": None\n","        }\n","\n","        prev_session_ids = None\n","\n","        for epoch in range(epochs):\n","            epoch_loss = 0\n","            self.gru.train()\n","            progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\", leave=False)\n","\n","            for i, (input, target, mask, session_ids) in enumerate(progress_bar):\n","                batch_size = input.size(0)  # Dynamically get batch size\n","\n","                # Initialize hidden state based on current batch size\n","                # self.hidden = torch.zeros(self.gru.num_layers, batch_size, self.gru.hidden_size).to(self.device)\n","                input, target, mask = input.to(self.device), target.to(self.device), mask.to(self.device)\n","\n","                # Reset hidden state if session has changed\n","                if prev_session_ids is not None:\n","                    reset_mask = (torch.tensor(session_ids) != torch.tensor(prev_session_ids)).to(self.device)\n","                    self.hidden[:, reset_mask, :] = 0\n","                prev_session_ids = session_ids\n","\n","                # Forward pass\n","                logits, self.hidden = self.gru(input, self.hidden)\n","                self.hidden = self.hidden.detach()\n","\n","                logits = logits.reshape(-1, logits.size(-1))  # [b*n, 500180]\n","                # print(logits.shape)\n","                target = target.reshape(-1)  # [b*n]\n","                # print(target.shape)\n","                loss = (self.loss(logits, target) * mask).sum() / mask.sum()\n","\n","                # Backpropagation\n","                self.gru.optimizer.zero_grad()\n","                loss.backward()\n","                self.gru.optimizer.step()\n","\n","                batch_loss = loss.item()\n","                epoch_loss += batch_loss\n","\n","                # Update tqdm bar with loss\n","                progress_bar.set_postfix(loss=f\"{batch_loss:.4f}\")\n","\n","            train_losses.append(epoch_loss / len(train_loader))\n","\n","            # Validation\n","            val_loss, val_metrics = self.test(val_loader, 20)\n","            val_losses.append(val_loss)\n","\n","            # Append metrics\n","            for metric, value in val_metrics.items():\n","                if metric not in val_metrics or not isinstance(val_metrics[metric], list):\n","                    val_metrics[metric] = []\n","                val_metrics[metric].append(value)\n","\n","            # Print metrics\n","            print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {epoch_loss/len(train_loader):.4f},Val Loss: {val_loss:.4f}, Metric: {val_metrics}\")\n","\n","        return train_losses, val_losses, test_metrics\n","\n","    def test(self, test_loader, k=10):\n","        self.gru.eval()\n","        test_metrics = {\n","            \"mrr_at_k\": None,\n","            \"hit_at_k\": None\n","        }\n","\n","        total_loss = 0\n","        with torch.no_grad():\n","\n","            for input, target, mask, session_ids in tqdm(test_loader):\n","                batch_size = input.size(0)\n","                hidden = torch.zeros(self.gru.num_layers, batch_size, self.gru.hidden_size).to(self.device)\n","\n","                # Initialize hidden state based on current batch size\n","                input, target, mask = input.to(self.device), target.to(self.device), mask.to(self.device)\n","                logits, _ = self.gru(input, hidden)\n","                logits = logits.reshape(-1, logits.size(-1))  # [b*n, 500180]\n","                target = target.reshape(-1)\n","                # target = target.squeeze(-1)  # Ensure it has shape [batch_size]\n","                if target.dim() > 1:\n","                    target = target[:, 0]  # Take first element if needed\n","\n","                loss = (self.loss(logits, target) * mask).sum() / mask.sum()\n","                total_loss += loss.item()\n","\n","                # calculate for each metrics\n","                for i in range(logits.shape[0]):\n","                    scores_np = logits[i].cpu().numpy()\n","                    target_np = target[i].cpu().numpy()\n","                    evaluation = self.evaluate(scores_np, target_np, k)\n","                    total_relevant_label = np.where(pred == label).sum\n","\n","                    first_relevant_rank = None\n","                    for row in range(pred.size(0)):\n","                        top_k_preds = np.argsort(scores_np[row], axis=0, order=\"desc\")[:k]\n","                        if label[row].item() in top_k_preds:\n","                            if first_relevant_rank is None:\n","                                first_relevant_rank = np.where(top_k_preds == label[row].item())[0] + 1\n","                            top_k_correct += 1\n","                    mrr_at_k += 1 / first_relevant_rank if first_relevant_rank is not None else 0\n","        avg_loss = total_loss / len(test_loader)\n","        return avg_loss, test_metrics\n","\n","    def evaluate(self, pred, true, k):\n","        \"\"\"\n","        Optimized Hit@K and MRR@K calculation using batch-wise computation.\n","        \"\"\"\n","        pred = pred.cpu().numpy()  # Move to CPU once\n","        true = true.cpu().numpy()\n","\n","        # Get top-k predictions in one operation\n","        top_k_indices = np.argpartition(pred, -k, axis=1)[:, -k:]\n","        top_k_sorted = top_k_indices[np.argsort(pred[np.arange(pred.shape[0])[:, None], top_k_indices], axis=1)[:, ::-1]]\n","\n","        # Compute Hits@K efficiently\n","        hits = np.isin(true[:, None], top_k_sorted)  # Boolean mask for hits\n","        hit_at_k_score = hits.mean() * 100  # Convert to percentage\n","\n","        # Compute MRR@K efficiently\n","        ranks = np.where(hits, np.argsort(hits, axis=1) + 1, 0)  # Rank of true label\n","        reciprocal_ranks = np.where(ranks > 0, 1 / ranks, 0)\n","        mrr_at_k_score = reciprocal_ranks.mean() * 100  # Convert to percentage\n","\n","        return float(hit_at_k_score), float(mrr_at_k_score)\n"]},{"cell_type":"markdown","metadata":{"id":"QDGVjbdRriXg"},"source":["## Tes perulangan *training*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Aw4k80WNr1dA"},"outputs":[],"source":["batch_size = 8\n","\n","train_session_ids = [i for i in range(len(train_sequences))]\n","val_session_ids = [i for i in range(len(train_sequences), len(train_sequences) + len(val_sequences))]\n","\n","\n","train_dataset = SessionDataset(train_sequences, train_session_ids)\n","val_dataset = SessionDataset(val_sequences, val_session_ids)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YSAFMHqbIHoO"},"outputs":[],"source":["train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n","val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":458},"id":"y-odFIsOrlr3","outputId":"7f4dc260-da81-47d7-b814-891dafc653b1","executionInfo":{"status":"error","timestamp":1743464225990,"user_tz":-420,"elapsed":6684391,"user":{"displayName":"Nazhif Haidar","userId":"10254950625537949520"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["500180\n"]},{"output_type":"stream","name":"stderr","text":["  0%|          | 0/14778 [00:00<?, ?it/s]<ipython-input-14-84fd3a63a61b>:142: RuntimeWarning: divide by zero encountered in divide\n","  reciprocal_ranks = np.where(ranks > 0, 1 / ranks, 0)\n"," 24%|██▍       | 3562/14778 [18:48<59:14,  3.16it/s]\n"]},{"output_type":"error","ename":"IndexError","evalue":"index 19 is out of bounds for axis 0 with size 16","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-17-b680ff45fde3>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# test train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgru4rec_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-14-84fd3a63a61b>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_loader, val_loader, epochs)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;31m# Validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m             \u001b[0mval_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-14-84fd3a63a61b>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(self, test_loader, k)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m                 \u001b[0;31m# calculate for each metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m                 \u001b[0mevaluation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m                 \u001b[0mtest_metrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"hit_at_k\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m                 \u001b[0mtest_metrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"mrr_at_k\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-14-84fd3a63a61b>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, pred, true, k)\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;31m# Get top-k predictions in one operation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0mtop_k_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margpartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m         \u001b[0mtop_k_sorted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtop_k_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_k_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;31m# Compute Hits@K efficiently\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: index 19 is out of bounds for axis 0 with size 16"]}],"source":["output_size = label_encoder.classes_.shape[0]\n","print(output_size)\n","\n","gru4rec_model = GRU4REC(\n","    input_size=label_encoder.classes_.shape[0],\n","    hidden_size=128,\n","    output_size=output_size,\n","    num_layers=2,\n","    dropout_hidden_rate=0.2,\n","    dropout_input_rate=0.2,\n","    batch_size=batch_size,\n","    optimizer=None,\n","    loss=nn.CrossEntropyLoss()\n",")\n","\n","gru4rec_model.gru.optimizer = Adam(gru4rec_model.gru.parameters(), lr=0.001)\n","\n","# test train\n","train_loss, val_loss, test_metrics = gru4rec_model.train(train_loader, val_loader, epochs=20)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ruvNRcFUIX8k"},"outputs":[],"source":["# import gc\n","# del gru4rec_model\n","# gc.collect()\n","# torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pAKdTb6wgy0o"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"authorship_tag":"ABX9TyM4mEf1nIMTjXXL7KEeU3Ut"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}